<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Week 1 The Eye-tracking Method and its Application in Language Research</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/white-cus.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section>
                    <h2>Week 1</h2>
                    <h3>Introduction to the Human Visual System and the Eye-tracking Method</h3>
                    <!-- <p><img src="assets/feedback-day-1.png" width="15%"></p> -->
                </section>
                <section>
                    <img src="./assets/eye-tracking-papers.jpg"  width="70%">
                </section>
				<section>
                    <h3>Today's roadmap</h3>
                    <ul>
                        <li class="fragment">The basics of eye-tracking</li>
                        <li class="fragment">Eye-tracking and visual attention</li>
                        <li class="fragment">Application of the eye-tracking method in language research: an overview</li>
                    </ul>
                </section>
                <section data-auto-animate data-auto-animate-id="2">
                    <h3>The basics of eye-tracking</h3>
                </section>
                <section data-auto-animate data-auto-animate-id="2">
                    <h3>The basics of eye-tracking</h3>
                    <h4>The human visual system</h4>
                    <img src="./assets/eye.png"  width="40%">
                </section>
                <section data-auto-animate data-auto-animate-id="2">
                    <h3>The basics of eye-tracking</h3>
                    <h4>The human visual system</h4>
                    <img src="./assets/eye.png"  width="40%"> <img src="./assets/visual-acuity.png"  width="40%">
                </section>
                <section data-auto-animate data-auto-animate-id="2">
                    <h3>The basics of eye-tracking</h3>
                    <h4>The human visual system</h4>
                    <ul>
                        <li class="fragment">Cones - photorecepter cells responsible for detailed vision - are the most dense at the fovea.</li>
                        <li class="fragment">(Rods - responsible for dim-light vision and night vision.)</li>
                        <li class="fragment">Visual acuity is the highest around the fovea.</li>
                        <li class="fragment">To compensate for lessened acuity outside the fovea, we must frequently move our eyes .</li>
                    </ul>
                </section>
                <section data-auto-animate data-auto-animate-id="3">
                    <h3>The basics of eye-tracking</h3>
                    <h4>Types of eye movements</h4>
                </section>
                <section data-auto-animate data-auto-animate-id="3">
                    <h3>The basics of eye-tracking</h3>
                    <h4>Types of eye movements</h4>
                    <ul>
                        <li>Fixations</li>
                        <li>Saccades</li>
                        <li>Smooth pursuits</li>
                    </ul>
                </section>
                <section data-auto-animate data-auto-animate-id="3">
                    <h3>The basics of eye-tracking</h3>
                    <h4>Types of eye movements</h4>
                    <ul>
                        <li>Fixations</li>
                        <li>Saccades</li>
                        <li>Smooth pursuits</li>
                    </ul>
                    <video controls src="./assets/fixations-and-saccades.webm" width="40%"></video>
                </section>
                <section data-auto-animate data-auto-animate-id="3">
                    <h3>The basics of eye-tracking</h3>
                    <h4>Types of eye movements</h4>
                    <ul>
                        <li>Fixations</li>
                        <li>Saccades</li>
                        <li>Smooth pursuits</li>
                    </ul>
                    <video controls src="./assets/smooth-pursuit.webm" width="40%"></video>
                </section>
                <section data-auto-animate data-auto-animate-id="4">
                    <h3>The basics of eye-tracking</h3>
                    <h4>Modern eye-tracking techniques</h4>
                </section>
                <section data-auto-animate data-auto-animate-id="4">
                    <h3>The basics of eye-tracking</h3>
                    <h4>Modern eye-tracking techniques</h4>
                    <p>Two types of eye movement measurements:</p>
                    <ul>
                        <li class="fragment">The position of the eye relative to the head</li>
                        <li class="fragment">The point of regard (the orientation of the eye in space)</li>
                    </ul>
                </section>
                <section data-auto-animate data-auto-animate-id="4">
                    <h3>The basics of eye-tracking</h3>
                    <h4>Modern eye-tracking techniques</h4>
                    <p>Two types of eye movement measurements:</p>
                    <ul>
                        <li>The position of the eye relative to the head</li>
                        <li><b>The point of regard (the orientation of the eye in space)</b></li>
                    </ul>
                </section>
                <section data-auto-animate data-auto-animate-id="4">
                    <h3>The basics of eye-tracking</h3>
                    <h4>Modern eye-tracking techniques</h4>
                    <p>Modern techniques to measure the point of regard:</p>
                    <ul>
                        <li class="fragment">Video-based corneal reflection eye trackers
                            <ul>
                                <li class="fragment">Corneal-reflection (of an infra-red light source)</li>
                                <li class="fragment">Pupil center</li>
                            </ul>
                        </li>
                    </ul>
                </section>
                <section data-auto-animate data-auto-animate-id="4">
                    <h3>The basics of eye-tracking</h3>
                    <h4>Modern eye-tracking techniques</h4>
                    <p>Modern techniques to measure the point of regard:</p>
                    <ul>
                        <li>Video-based corneal reflection eye trackers</li>
                    </ul>
                    <p><img src="./assets/eyelink-setup.jpg"  width="40%"> <img src="./assets/eyelink-camera.png"  width="40%"></p>
                </section>
                <section data-auto-animate data-auto-animate-id="4">
                    <h3>The basics of eye-tracking</h3>
                    <h4>Modern eye-tracking techniques</h4>
                    <p>Modern techniques to measure the point of regard:</p>
                    <ul>
                        <li>Video-based corneal reflection eye trackers</li>
                    </ul>
                    <p><img src="./assets/eyelink-camera.png"  width="40%"><img src="./assets/eyelink-pcr.jpg"  width="40%"></p>
                </section>
                <section data-auto-animate data-auto-animate-id="4">
                    <h3>The basics of eye-tracking</h3>
                    <h4>Modern eye-tracking techniques</h4>
                    <p><img src="./assets/sample-data-freeview.webp"  width="70%"></p>
                </section>
                <section data-auto-animate data-auto-animate-id="4">
                    <h3>The basics of eye-tracking</h3>
                    <h4>Modern eye-tracking techniques</h4>
                    <p>Going beyond eye movements:</p>
                    <ul>
                        <li class="fragment">Modern eye trackers can usually provide more information than the point of regard (e.g. pupil size)</li>
                        <li style="list-style-type: none; text-align: center;" class="fragment"><img src="./assets/pupillometry.jpg"  width="40%"></li>
                        <li class="fragment">These measures are being used in psychology-related research fields including language</li>
                    </ul>
                </section>
                <section data-auto-animate data-auto-animate-id="5">
                    <h3>Eye-tracking and visual attention</h3>
                </section>
                <section data-auto-animate data-auto-animate-id="5">
                    <h3>Eye-tracking and visual attention</h3>
                    <ul>
                        <li class="fragment">Eye movements are highly correlated with visual attention</li>
                        <li class="fragment">Attention allows us to selectively process information</li>
                        <li class="fragment">By tracking eye movement, we follow the observer's visual attention...</li>
                        <li class="fragment">...and gain insight of how the observer is processing the scene</li>
                    </ul>
                </section>
                <section data-auto-animate data-auto-animate-id="5">
                    <h3>Eye-tracking and visual attention</h3>
                    <ul>
                        <li>
                            During free-viewing of a scene: fixations usually center on interesting/informative areas of the image
                        </li>
                    </ul>
                    <p><img src="./assets/wooding-fig2.png"  width="50%"></p>
                </section>
                <!-- <section data-auto-animate data-auto-animate-id="5">
                    <h3>Eye-tracking and visual attention</h3>
                    <ul>
                        <li>
                            During free-viewing of a scene: fixations usually center on interesting/informative areas of the image
                        </li>
                    </ul>
                    <p><video controls src="./assets/sample-data-freeview2.webm" width="65%"></video></p>
                </section>
                <section data-auto-animate data-auto-animate-id="5">
                    <h3>Eye-tracking and visual attention</h3>
                    <ul>
                        <li>
                            During free-viewing of a scene: fixations usually center on interesting/informative areas of the image
                        </li>
                    </ul>
                    <p><img src="./assets/sample-data-freeview2.webp"  width="65%"></p>
                </section> -->
                <section data-auto-animate data-auto-animate-id="5">
                    <h3>Eye-tracking and visual attention</h3>
                    <ul>
                        <li>
                            During free-viewing of a scene: fixations usually center on interesting/informative areas of the image
                        </li>
                    </ul>
                    <p><video controls src="./assets/sample-data-freeview3.webm" width="65%"></video></p>
                </section>
                <section data-auto-animate data-auto-animate-id="5">
                    <h3>Eye-tracking and visual attention</h3>
                    <ul>
                        <li>
                            During free-viewing of a scene: fixations usually center on interesting/informative areas of the image
                        </li>
                    </ul>
                    <p><img src="./assets/sample-data-freeview3.webp"  width="65%"></p>
                </section>
                <section data-auto-animate data-auto-animate-id="6">
                    <h3>Eye-tracking in language research:</h3>
                    <h3>a brief overview</h3>
                </section>
                <section data-auto-animate data-auto-animate-id="6">
                    <h3>Eye-tracking in language research</h3>
                    <p>Reading</p>
                    <p><video controls src="./assets/sample-data-reading2.webm" width="65%"></video></p>
                </section>
                <section data-auto-animate data-auto-animate-id="6">
                    <h3>Eye-tracking in language research</h3>
                    <p>Reading</p>
                    <p><img src="./assets/sample-data-reading2.webp"  width="70%"></p>
                </section>
                <section data-auto-animate data-auto-animate-id="6">
                    <h3>Eye-tracking in language research</h3>
                    <p>Reading</p>
                    <p><ul>
                        <li class="fragment">Fixation pattern during reading is affected by many factors such as legibility and syntactic/conceptual difficulty.</li>
                        <li class="fragment">Research on different levels of language processing:</li>
                        <li class="fragment">Single word; Sentence; Whole/Multiple texts</li>
                        <li class="fragment">Practical/Clinical applications.</li>
                    </ul></p>
                </section>
                <section data-auto-animate data-auto-animate-id="7">
                    <h3>Eye-tracking in language research</h3>
                    <p>Spoken language processing</p>
                    <p style="text-align: left;">The visual world paradigm</p>
                    <p><img src="./assets/sample-data-vwe.png"  width="65%"></p>
                </section>
                <section data-auto-animate data-auto-animate-id="7">
                    <h3>Eye-tracking in language research</h3>
                    <p>Spoken language processing</p>
                    <p style="text-align: left;" class="fragment">The visual world paradigm</p>
                    <p><ul>
                        <li class="fragment">Visual display + spoken language comprehension</li>
                        <li class="fragment">Activation of an object's label determines the probability of shifting attention to that object and thus making a saccadic eye movement to fixate it</li>
                        <li class="fragment">Continuous measure of cognition that has a fine temporal resolution</li>
                    </ul></p>
                </section>
                <!-- <section data-auto-animate data-auto-animate-id="7">
                    <h3>Eye-tracking in language research</h3>
                    <p>Spoken language processing</p>
                    <p style="text-align: left;">The visual world paradigm</p>
                    <p><ul>
                        <li class="fragment">Word recognition: proportion of fixations is sensitive to lexical stress, duration, pitch accents, etc.</li>
                        <li class="fragment">Sentence comprehension: visual world paradigm can be used to study syntactic parsing, semantic integration, discourse processing, etc.</li>
                    </ul></p>
                </section> -->
                <section data-auto-animate data-auto-animate-id="8">
                    <h3>Eye-tracking in language research</h3>
                    <p>Combining eye-tracking and neuroimaging</p>
                    <p style="text-align: left;" class="fragment">Electroencephalography (EEG)</p>
                    <p><ul>
                        <li class="fragment">Measuring the brain's electrical activity ("brain waves").</li>
                        <li class="fragment">High temporal resolution on the millisecond scale.</li>
                        <li class="fragment">The brain's response to an event (stimulus onset) is called Event-Related Potentials (ERPs).</li>
                    </ul></p>
                </section>
                <section data-auto-animate data-auto-animate-id="8">
                    <h3>Eye-tracking in language research</h3>
                    <p>Combining eye-tracking and neuroimaging</p>
                    <p style="text-align: left;">Electroencephalography (EEG)</p>
                    <p style="display: flex; justify-content: center; align-items: center;"><img src="./assets/biosemi-setup.gif" width="30%"><img src="./assets/kutas-federmeier-1999.png" width="45%"></p>
                </section>
                <section data-auto-animate data-auto-animate-id="8">
                    <h3>Eye-tracking in language research</h3>
                    <p>Combining eye-tracking and neuroimaging</p>
                    <p style="text-align: left;" class="fragment">Functional Magnetic Resonance Imaging (fMRI)</p>
                    <p><ul>
                        <li class="fragment">Measuring the brain's neural activity by measuring associated changes in blood flow.</li>
                        <li class="fragment">High spacial resolution on the millimeter scale.</li>
                    </ul></p>
                </section>
                <section data-auto-animate data-auto-animate-id="8">
                    <h3>Eye-tracking in language research</h3>
                    <p>Combining eye-tracking and neuroimaging</p>
                    <p style="text-align: left;">Functional Magnetic Resonance Imaging (fMRI)</p>
                    <p style="display: flex; justify-content: center; align-items: center;"><img src="./assets/siemens-mri.webp" width="45%"><img src="./assets/Szaflarski-2002.jpg" width="45%"></p>
                </section>
                <section data-auto-animate data-auto-animate-id="8">
                    <h3>Eye-tracking in language research</h3>
                    <p>Combining eye-tracking and neuroimaging</p>
                    <p><ul>
                        <!-- <li class="fragment">Eye-tracking results can be discussed together with neuroimaging results:</li>
                        <li class="fragment">Different techniques complement one another to advance science.</li> -->
                        <li class="fragment">Recently, the emergence of co-registration of eye movements and EEG/fMRI data allows researchers to measure neural activity in highly natural tasks:</li>
                        <li class="fragment">Eye movements provide information on what is being looked at, and EEG or fMRI data provide information on the brain's response to that visual information</li>
                    </ul></p>
                </section>
                <section data-auto-animate data-auto-animate-id="8">
                    <h3>Eye-tracking in language research</h3>
                    <p>Combining eye-tracking and neuroimaging</p>
                    <p style="text-align: left;" class="fragment">Co-registration of eye-tracking and EEG</p>
                    <p><ul>
                        <li class="fragment">Classic language-related ERP effects (e.g. the N400 effect) have been replicated during natural reading in fixation-related potentials (FRPs) (Dimigen et al., 2011).</li>
                    </ul></p>
                    <p style="display: flex; justify-content: center; align-items: center;"><img src="./assets/eeg-eye-tracker-integration-photoshop.jpg" width="45%" class="fragment"><img src="./assets/dimigen-2011.png" width="45%" class="fragment"></p>
                </section>
                <section data-auto-animate data-auto-animate-id="8">
                    <h3>Eye-tracking in language research</h3>
                    <p>Combining eye-tracking and neuroimaging</p>
                    <p style="text-align: left;" class="fragment">Co-registration of eye-tracking and fMRI</p>
                    <p><ul>
                        <li class="fragment">Concurrent eye-tracking and fMRI to investigate syntactic parsing and lexical activation during natural reading.</li>
                    </ul></p>
                    <p style="display: flex; justify-content: center; align-items: center;"><img src="./assets/eyelink-mri-example.jpg" width="45%" class="fragment"><img src="./assets/carter-2019.png" width="40%" class="fragment"></p>
                </section>
                <section data-auto-animate data-auto-animate-id="9">
                    <h3>Today's take-home message</h3>
                    <ul>
                        <li class="fragment">The two types of regular eye movement are fixations and saccades. </li>
                        <li class="fragment">Eye movement is closely associated with visual attention, thus can provide insight into cognition.</li>
                        <li class="fragment">Eye-tracking can be used to study both reading and spoken language processing.</li>
                        <li class="fragment">Recent developments of technology allows researchers to combine eye-tracking and neuroimaging. </li>
                    </ul>
                </section>
                <section data-auto-animate data-auto-animate-id="9">
                    <h3>Today's take-home message</h3>
                    <ul>
                        <li>The two types of regular eye movement are fixations and saccades. </li>
                        <li>Eye movement is closely associated with visual attention, thus can provide insight into cognition.</li>
                        <li>Eye-tracking can be used to study both reading and spoken language processing.</li>
                        <li>Recent developments of technology allows researchers to combine eye-tracking and neuroimaging. </li>
                    </ul>
                    <p><img src="assets/feedback-day-1.png" width="15%"></p>
                </section>
			</div>
            <div class="logo"></div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,
                slideNumber: 'c/t',

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes ]
			});
            Reveal.configure({ pdfSeparateFragments: false });
		</script>
	</body>
</html>
